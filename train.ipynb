{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pipeline import Pipeline\n",
    "import torch\n",
    "from transforms import pad_token\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import pickle\n",
    "import random\n",
    "from dataset import TBTTScriptsDataset, ScriptsDataset\n",
    "from model import Network, PackedNetwork\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, labels in tqdm_notebook(loader, desc = \"Validation Batches\", unit = \"batch\", leave = True):\n",
    "        batch_size, seq_len = data.shape\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        data_truncated = torch.chunk(data, int(seq_len / T), dim = 1)\n",
    "        for truncated_slice in data_truncated:\n",
    "            predicted, hidden = model(truncated_slice, hidden)\n",
    "            \n",
    "        total += labels.size(0)\n",
    "        correct += torch.mean((predicted - labels) ** 2)\n",
    "    return (correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"10k_common.pkl\", \"rb\") as f:\n",
    "#   data, token_idx, idx_token = pickle.load(f).data\n",
    "#   data = data.apply(lambda x: x[:500])\n",
    "  \n",
    "# with open(\"rating.pkl\", \"rb\") as f:\n",
    "#   ratings = pickle.load(f)\n",
    "\n",
    "data, token_idx, idx_token = Pipeline.load(\"10k_common\").data\n",
    "ratings = Pipeline.load(\"ratings\").data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, ratings, test_size=0.15)\n",
    "X_train, X_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "\n",
    "train_loader = ScriptsDataset(X_train, y_train).get_loader(batch_size = 32)\n",
    "val_loader = ScriptsDataset(X_test, y_test).get_loader(batch_size = 32)\n",
    "# train_loader = TBTTScriptsDataset(X_train, y_train).get_loader()\n",
    "# val_loader = TBTTScriptsDataset(X_test, y_test).get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Network(rnn_type = \"gru\", emb_size = 100, hidden_size = 128, num_layers = 1, vocab_size = len(idx_token), pad_idx = token_idx[pad_token])\n",
    "model = PackedNetwork(rnn_type = \"gru\", emb_size = 128, hidden_size = 156, num_layers = 1, vocab_size = len(idx_token), pad_idx = token_idx[pad_token])\n",
    "\n",
    "learning_rate = .00001\n",
    "num_epochs = 2\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "interval_loss = []\n",
    "for epoch in tqdm_notebook(range(num_epochs), desc = \"Training Epochs\", unit = \"epoch\"):\n",
    "    for i, (data, labels) in enumerate(tqdm_notebook(train_loader, desc = \"Batches\", unit = \"batch\")):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size, seq_len = data.shape\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        chunks = int(seq_len / T)\n",
    "        if chunks > 0:\n",
    "            data_truncated = torch.chunk(data, chunks, dim = 1)\n",
    "            set_trace()\n",
    "            for truncated_slice in data_truncated:\n",
    "                outputs, hidden = model(truncated_slice, hidden)\n",
    "        else:\n",
    "            outputs, hidden = model(data, hidden)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        \n",
    "#         enc_grads = torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        losses.append(loss.item())\n",
    "        interval_loss.append(loss.item())\n",
    "        if i > 0 and i % 300 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            avg_intval_loss = sum(interval_loss) / len(interval_loss)\n",
    "            tqdm.write('Epoch: [{}/{}], Step: [{}/{}], Average MSE: {:.4f}, Avg Loss: {:.4f}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc, avg_intval_loss))\n",
    "            interval_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662688a4a2004659b7d2497fb78882b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Epochs', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebf364b63054930b237b588e81d15df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Sample', max=3519), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e02df0137648af80abad69a9cf9a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validation Sample', max=621), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k1 = 8 # interval\n",
    "k2 = 12 # sequence length\n",
    "retain_graph = k1 < k2\n",
    "\n",
    "losses = []\n",
    "interval_loss = []\n",
    "\n",
    "for epoch in tqdm_notebook(range(num_epochs), desc = \"Training Epochs\", unit = \"epoch\"):\n",
    "    for i, (seq, target) in enumerate(tqdm_notebook(zip(X_train, y_train), desc = \"Training Sample\", unit = \"sample (k1 = {})\".format(k1), total = len(X_train), miniters=k1)):\n",
    "        model.train()\n",
    "        \n",
    "        outputs = []\n",
    "        targets = []\n",
    "        \n",
    "        hidden = model.init_hidden(1)\n",
    "        states = [(None, hidden)]\n",
    "        \n",
    "        state = states[-1][1].detach()\n",
    "        state.requires_grad=True\n",
    "\n",
    "        seq = torch.Tensor(seq).long().view(1, -1)\n",
    "        output, new_state = model(seq, state)\n",
    "        states.append((state, new_state))\n",
    "\n",
    "        outputs.append(output)\n",
    "        targets.append(torch.tensor([target]))\n",
    "\n",
    "        while len(outputs) > k1:\n",
    "            # Delete stuff that is too old\n",
    "            del outputs[0]\n",
    "            del targets[0]\n",
    "\n",
    "        while len(states) > k2:\n",
    "            # Delete stuff that is too old\n",
    "            del states[0]\n",
    "\n",
    "        if (i+1) % k1 == 0:\n",
    "            optimizer.zero_grad()\n",
    "            # backprop last module (keep graph only if they ever overlap)\n",
    "            for j in range(k2-1):\n",
    "                if j < k1:\n",
    "                    loss = criterion(outputs[-j-1], targets[-j-1])\n",
    "                    loss.backward(retain_graph=retain_graph)\n",
    "\n",
    "                # if we get all the way back to the \"init_state\", stop\n",
    "                if states[-j-2][0] is None:\n",
    "                    break\n",
    "                curr_grad = states[-j-1][0].grad\n",
    "                states[-j-2][1].backward(curr_grad, retain_graph=retain_graph)\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            interval_loss.append(loss.item())\n",
    "            if (i+1) % (k1 * 35) == 0:\n",
    "                # validate\n",
    "                errs = []\n",
    "                model.eval()\n",
    "                for val_seq, val_targ in tqdm_notebook(zip(X_test, y_test), desc = \"Validation Sample\", unit = \"sample\", total = len(X_test), leave = False):\n",
    "                    val_seq = torch.Tensor(val_seq).long().view(1, -1)\n",
    "                    out, h = model(val_seq, model.init_hidden(val_seq.shape[0]))\n",
    "                    errs.append((out - val_targ) ** 2)\n",
    "                \n",
    "                avg_err = torch.mean(torch.Tensor(errs))\n",
    "                avg_intval_loss = torch.mean(torch.Tensor(interval_loss))\n",
    "                \n",
    "                tqdm.write('Epoch: [{}/{}], Step: [{}/{}], Average MSE: {:.4f}, Avg Loss: {:.4f}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(X_train), avg_err, avg_intval_loss))\n",
    "                \n",
    "                interval_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "interval_loss = []\n",
    "for epoch in tqdm_notebook(range(num_epochs), desc = \"Training Epochs\", unit = \"epoch\"):\n",
    "    for i, (data, lengths, labels) in enumerate(tqdm_notebook(train_loader, desc = \"Batches\", unit = \"batch\")):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size, seq_len = data.shape\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "#         hidden_transitions = [(None, hidden)]\n",
    "        \n",
    "        chunks = int(seq_len / T)\n",
    "        if chunks > 0:\n",
    "            data_truncated = torch.chunk(data, chunks, dim = 1)\n",
    "#             set_trace()\n",
    "            for i, truncated_slice in enumerate(data_truncated):\n",
    "                slice_lengths = (truncated_slice != 0).sum(1)\n",
    "                outputs, hidden = model(truncated_slice, slice_lengths, hidden)\n",
    "                \n",
    "#                 hidden_transitions.append((hidden_transitions[-1][1], hidden))\n",
    "                if i < len(data_truncated) - 2:\n",
    "                    hidden.backward()\n",
    "                    hidden.detach()\n",
    "        else:\n",
    "            outputs, hidden = model(data, hidden)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        \n",
    "#         enc_grads = torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        losses.append(loss.item())\n",
    "        interval_loss.append(loss.item())\n",
    "        if i > 0 and i % 300 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            avg_intval_loss = sum(interval_loss) / len(interval_loss)\n",
    "            tqdm.write('Epoch: [{}/{}], Step: [{}/{}], Average MSE: {:.4f}, Avg Loss: {:.4f}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc, avg_intval_loss))\n",
    "            interval_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# interval_loss = []\n",
    "# for epoch in tqdm_notebook(range(num_epochs), desc = \"Training Epochs\", unit = \"epoch\"):\n",
    "#     for i, (data, labels) in enumerate(tqdm_notebook(train_loader, desc = \"Batches\", unit = \"batch\")):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         batch_size, seq_len = data.shape\n",
    "#         hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "#         chunks = int(seq_len / T)\n",
    "#         if chunks > 0:\n",
    "#             data_truncated = torch.chunk(data, chunks, dim = 1)\n",
    "#             for truncated_slice in data_truncated:\n",
    "#                 outputs, hidden = model(truncated_slice, hidden)\n",
    "#         else:\n",
    "#             outputs, hidden = model(data, hidden)\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         loss.backward()\n",
    "        \n",
    "# #         enc_grads = torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "        \n",
    "#         optimizer.step()\n",
    "                \n",
    "#         losses.append(loss.item())\n",
    "#         interval_loss.append(loss.item())\n",
    "#         if i > 0 and i % 300 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader, model)\n",
    "#             avg_intval_loss = sum(interval_loss) / len(interval_loss)\n",
    "#             tqdm.write('Epoch: [{}/{}], Step: [{}/{}], Average MSE: {:.4f}, Avg Loss: {:.4f}'.format(\n",
    "#                        epoch+1, num_epochs, i+1, total_step, val_acc, avg_intval_loss))\n",
    "#             interval_loss = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
